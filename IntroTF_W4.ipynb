{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Tensorflow for AI - Week 4\n",
    "---\n",
    "This notebook contains exercises for week 4 of the [Coursera Course](https://www.coursera.org/learn/introduction-tensorflow/lecture/zOIps/a-conversation-with-andrew-ng) Introduction into Tensorflow for AI Development\n",
    "\n",
    "So far we are still working with image classifications.  In this case we are looking at perhaps some more complicated data sets (less ideal images).\n",
    "\n",
    "---\n",
    "\n",
    "Tensorflow's image generator has a really cool feature by which it can scan directory tree structure to identify training and validation data sets and image labels by sub directories within each training and validation set. Neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating our data sets\n",
    "train_dir = '/home/Documets/Python'\n",
    "validation_dir = '/home/Documents/Python'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) # Buidling in a normalization\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300,300), # Resizing our images for us\n",
    "    batch_size=128, # This is importat to keep track of\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ryan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 298, 298, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 147, 147, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 71, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 78400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               40141312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 40,165,409\n",
      "Trainable params: 40,165,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural Network for Horses vs Humans\n",
    "\n",
    "# Our images are larger so we need more convolution and pooling.\n",
    "# Since these images are in color we have a 3 in our third dimension\n",
    "# for our initial convolution (red, green, blue bits).\n",
    "\n",
    "# Sigmoid is a good activation function for binary classifiers.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu',\n",
    "                          input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Binary, only one neuron\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a fairly impressive model, reducing our input from over 900,000 vector length to just 78,400, we need to compile it.  We will use slightly different optimizers and loss functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=RMSprop(lr=0.001),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be using a generator to build our training and test data sets, we use a fitting function that expects a generator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8, # This is determined by the batch size of your train_generator and the size of the data\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8, # this too is determined by batch size in generators\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
